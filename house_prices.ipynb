{"cells":[{"metadata":{},"cell_type":"markdown","source":"# House prices \n\n* trying to learn with help of Pedro Marcelinos notebook link: https://www.kaggle.com/pmarcelino/comprehensive-data-exploration-with-python#'SalePrice',-her-buddies-and-her-interests\nCopied a lot by hand as a way to learn. ","execution_count":null},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm \nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats \nimport warnings \nwarnings.filterwarnings('ignore')\n%matplotlib inline\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# todo train has the prices already filled in, using the train dataset we are finding the house prices of the test dataset. write smthing like this \ndf_train = pd.read_csv('../input/house-prices-advanced-regression-techniques/train.csv')\ndf_test = pd.read_csv('../input/house-prices-advanced-regression-techniques/test.csv')","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"df_train.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.columns","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Introduction brainstorm\n\nTry and get overview of the data\n\nWhat do i value in a house? \n\nChoose some columns that I want to focus on. \n\nFocus on:\n* SalePrice (the main focus) \n* OverallQual (not sure how it is computed, maybe some sort of average of the other columns conditions)\n* YearBuilt \n* TotalBsmtSF\n* GrLivArea\n* location??","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train.describe()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_train['SalePrice'].describe()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Look at how sale price relates to some other intresting columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#histogram \nsns.distplot(df_train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Sales price relative to numerical variables ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# skewness - todo\n# and\n# kurtosis - todo \nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Relation between SalePrice and some other columns","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot grlivarea/saleprice\n# todo describe what you see from this graph pls \nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By looking at the graph I can see that in most cases a bigger GrLivArea (ground living area???) means that is has a higher SalePrice.\nIt kind of follows a l**inear relationship**","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#scatter plot totalbsmtsf/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This graph looks to follow more of a exponential shape. \nThere are some outlier values, so it could be that the TotalBsmtSF is big but the house has some other flaws that is bringing down the total sales price. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"Looking at categorical features ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#box plot overallqual/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"can see that for the most part the higher the qualiity the more the house costs. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"var = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16,8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\nplt.xticks(rotation=90)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Can't see a huge differens on the price depending on what year it was built, but I would say that the newer houses tend to cost more. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"small summary - seems like the house prices behave as you mostly would expect. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Heatmaps gives a quick overview of the column relations. \ncan see some other interesting columns that have close relation to the sales price that are worth exploring closer...","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"From this we can see more clearly which columns that has a close relation to SalePrice. \nGarageCars and GarageArea impact the SalePrice, they are kind of the same since amout of cars that fit into the garage is depends on how big the garage area is. \nWhether the house has a full bath seem to matter. \nChoosing to discard 'duplicates' such as _______","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# scatterplots between \"SalePrice\" and correlated variables\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Observation: TotalBsmtSF and GrLivArea, can see a linear line which makes sense because usually basements are the same area as ground living area and not often bigger. Which the graph shows. \nIn this scatter plot of SalePrice and YearBuilt i think it shows more clearly that when the house is built more recently the price tend to be more expensive. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### Missing data \n* How prevalent(accepted) is the missing data? \n* Is missing data random or does it have a pattern? ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# missing data \ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"When should be consider a column irrelevant? In this case we choose when more than 15 % of the data is missing, that column will be deleted. \nBy looking at which columns that are missing the most data, we can see that these columns are not what you typically value when looking for a house. \nNote that the Garage___ columns has the same missing percentage. \nDeciding to delete all variables with missing data except 'Electrical' where we only will delte the observation with missing data. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#missing data\ndf_train = df_train.drop((missing_data[missing_data['Total'] > 1]).index,1)\ndf_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\ndf_train.isnull().sum().max() # doublecheck to make sure no missing data remains ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Outliers \nAre important and can provide us with good information.\nGoing to use a quick approch with standard deviation of 'SalePrice' and a set of scatter plots. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Univariate analysis\nLooking at one variable at a time. \nWill standardize the data by converting data values to have mean of 0 and a standard deviation of 1. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#standardizing data \nsaleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis])\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\n outer range (high) of the distribution:')\nprint(high_range)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Low range values are similar and not too far from 0.\n* High range values arre far from 0 and the 7 something values are really out of range.\n\nFor now, we'll not consider any of these values as an outlier but we should be careful with those two 7 something values. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"##### Bivariate analysis\nGoing to take a look at previous scatterplots with a new perspective. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"#bivariate analysis saleprice/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Revealed:\n* Found two values (bigger 'GrLivArea') that are not following the rest. It could be that they are located in a house on the countryside. Regardless they do not follow the typical case so we call them outliers and will delete them. \n\n* The two highest SalePrice points are a bit out from the rest but still seem to follow the trend so we will keep them. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# deleting points \ndf_train.sort_values(by= 'GrLivArea', ascending = False)[:2]\ndf_train = df_train.drop(df_train[df_train['Id'] == 1299].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 524].index)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# bivariate analysis saleprice/grlivarea\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"You might be tempted to delete the TotalBsmtSf > 3000 points but decided to leave them in. ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"### How is SalePrice calculated? ","execution_count":null},{"metadata":{},"cell_type":"markdown","source":"testing four assumptions:\n\nNormality: The data should follow a normal distribution. \n\nHomoscendasticity: refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)'. Homoscedasticity is desirable because we want the error term to be the same across all values of the independet variables. \n\nLinearity: The most common way is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships. \n\nAbscence of correlated errors: They happen when one error is correlated to another. For instance, if one positive error makes a negative error systematiaclly, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. When getting this error, try to add a variable that can explain the effect you are getting. That's the most common solution for correlated errors.","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# Exploring normality\n# histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line. \n\nBut we can use data transformation to solve the problem. There is a 'trick' where in the case of positive skewness, log transformations usually works well. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# applying log transformation\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the transformed histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Great! Now lets see 'GrLivArea'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# 'GrLivArea' histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# It is skewed so... data transformation\ndf_train['GrLivArea'] = np.log(df_train['GrLivArea'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# the transformed histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm)\nfig =plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On to the next couple! ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram and normal probability plot\nsns.distplot(df_train['TotalBsmtSF'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train['TotalBsmtSF'], plot = plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"* Seeing skewness.\n* Some zero values indicating houses without basements.\n* Zero values preventing the log transformation to take place. \n\nThere is a way to work through this problem. We will create a variable taht can get the effect of having or not having a basement (binary variable). Then we will do log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having a basement or not. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# create column for new variable (one is enough because it's a binary categorical feature)\n# if area > 0 it gets 1, if area is == 0 it gets 0\ndf_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)\ndf_train['HasBsmt'] = 0\ndf_train.loc[df_train['TotalBsmtSF'] > 0, 'HasBsmt'] = 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# transform data\ndf_train.loc[df_train['HasBsmt'] == 1, 'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# histogram and normal probability plot\nsns.distplot(df_train[df_train['TotalBsmtSF'] > 0]['TotalBsmtSF'], fit=norm)\nfig = plt.figure()\nres = stats.probplot(df_train[df_train['TotalBsmtSF'] > 0]['TotalBsmtSF'], plot=plt)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"#### Homoscedasticity\n\nThe best approch to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, larger dispersion at the opposite side) or diamonds ( a large number of points at the center of the distribution).","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# starting by SalePrice and GrLivArea\n# scatter plot\nplt.scatter(df_train['GrLivArea'], df_train['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The scatter plot before log transformations, had a conic shape. Now it does'nt have a conic shape, thats the power of normality. ","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# SalePrice with TotalBsmtSF\n# scatter plot\nplt.scatter(df_train[df_train['TotalBsmtSF'] > 0]['TotalBsmtSF'], df_train[df_train['TotalBsmtSF'] > 0]['SalePrice'])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"With this we can say in general that 'SalePrice' exhibit equal levels of variance across the range of 'TotalBsmtSF'","execution_count":null},{"metadata":{"trusted":true},"cell_type":"code","source":"# covert categorical variable into dummy\n# dummy variables are categorical data turned into 1 or 0 for simplicity. \ndf_train = pd.get_dummies(df_train)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}